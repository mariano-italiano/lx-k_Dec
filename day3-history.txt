kubectl get pods
kubectl get nodes
kubectl get pods -o wide
kubectl delete pod busybox init-pod2
kubectl get job
kubectl delete job my-job 
kubectl get pods -o wide
kubectl describe deployments.apps nginx-deployment 
kubectl edit deployments.apps nginx-deployment 
kubectl create deployment  test --image nginx  -o yaml --dry-run=client
kubectl create deployment  test --image nginx  -o yaml --dry-run=server
kubectl edit deployments.apps nginx-deployment 
kubectl rollout history deployment nginx-deployment 
kubectl describe deployments.apps nginx-deployment 
kubectl scale deployment nginx-deployment --replicas 8
kubectl get pods
kubectl rollout restart deployment nginx-deployment 
watch -n1 kubectl get pods
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml 
kubectl get all -n ingress-nginx
kubectl get pods -o wide
kubectl -n ingress-nginx edit svc ingress-nginx-controller
kubectl get all -n ingress-nginx
kubectl get svc
kubectl create deployment books --image nginx --replicas 3
kubectl create deployment cars --image nginx --replicas 3
kubectl expose deployment cars --name cars-svc --port 80 
kubectl expose deployment books --name books-svc --port 80 
kubectl get svc,deploy,pods
kubectl get svc,ep,pods
kubectl get svc,ep,pods -o wide
vi nginx-ingress.yaml
kubectl get ingressclass
vi nginx-ingress.yaml
vi cars-index.html
vi books-index.html
kubectl scale deployment cars replicas=1
kubectl scale deployment cars --replicas=1
kubectl scale deployment books --replicas=1
kubectl get pod
kubectl cp --help
kubectl cp cars-index.html cars-6985f6dbf7-qnjdt:/usr/share/nginx/html/index.html
kubectl cp books-index.html books-77864968dd-ms7xz:/usr/share/nginx/html/index.html
kubectl get svc 
kubectl apply -f nginx-ingress.yaml 
kubectl describe ingress nginx-ingress 
kubectl edit ingress nginx-ingress 
kubectl describe ingress nginx-ingress 
vi nodename-pod.yaml
kubectl get pods
kubectl get pods -o wide
kubectl get pods -o wide | grep worker01 | wc -l
kubectl get pods -o wide | grep worker02 | wc -l
vi nodename-pod.yaml
kubectl apply -f nodename-pod.yaml
Q
vi nodename-pod.yaml
kubectl apply -f nodename-pod.yaml
kubectl get pods -o wide
cp nodename-pod.yaml nodeselector-pod.yaml
vi nodeselector-pod.yaml
kubectl apply -f nodeselector-pod.yaml
vi nodeselector-pod.yaml
kubectl apply -f nodeselector-pod.yaml
kubectl get pods -o wide
kubectl events
kubectl get nodes 
kubectl get nodes --show-labels 
kubectl label nodes kubernetes.io/env=dev
kubectl label node worker01 kubernetes.io/env=dev
kubectl get pods -o wide
for i in worker1 worker2 k8s-master ; do kubectl describe node $i | grep -i taint; done
for i in worker01 worker02 k8s-master ; do kubectl describe node $i | grep -i taint; done
kubectl get nodes -o json | jq -r '.items[] | "Node name: \(.metadata.name), Taints: \(.spec.taints[0])"'
kubectl taint node worker02 zone=secure:NoSchedule
kubectl get nodes -o json | jq -r '.items[] | "Node name: \(.metadata.name), Taints: \(.spec.taints[0])"'
kubectl create deployment test-taints --image nginx --replicas 7 
kubectl get pods -o wide
kubectl edit deployments.apps test-taints 
kubectl get pods -o wide
kubectl edit pod init-pod 
watch -n1 kubectl get pods -o wied
watch -n1 kubectl get pods -o wide
kubectl get nodes -o json | jq -r '.items[] | "Node name: \(.metadata.name), Taints: \(.spec.taints[0])"'
kubectl taint node worker02 zone-
kubectl taint node worker02 region-
kubectl get nodes -o json | jq -r '.items[] | "Node name: \(.metadata.name), Taints: \(.spec.taints[0])"'
vi with-node-affinity.yaml
kubectl get nodes --show-labels 
vi with-node-affinity.yaml
kubectl apply -f with-node-affinity.yaml
kubectl get pods
kubectl label node worker01 kubernetes.io/country=poland; kubectl label node worker02 kubernetes.io/country=france
kubectl get pods
kubectl get pods -o wide
kubectl get pods --show-labels 
kubectl edit pod with-node-affinity 
kubectl get pods --show-labels 
vi with-pod-affinity.yaml
kubectl apply -f with-pod-affinity.yaml
kubectl get pods --show-labels -o wide
kubectl get pods -o wide
vi with-pod-anti-affinity.yaml
kubectl apply -f with-pod-anti-affinity.yaml
kubectl get pods -o wide
vi readiness-pod.yaml
kubectl apply -f readiness-pod.yaml
kubectl delete -f readiness-pod.yaml
vi readiness-pod.yaml
kubectl delete -f readiness-pod.yaml
kubectl apply -f readiness-pod.yaml
kubectl get pods
watch -n1 kubectl get pods
kubectl describe pod readiness-pod 
vi readiness-pod.yaml 
kubectl replace --force -f readiness-pod.yaml
watch -n1 kubectl get pods
vi readiness-pod.yaml 
kubectl describe pod readiness-pod 
vi readiness-pod-advanced.yaml
kubectl apply -f readiness-pod-advanced.yaml
watch -n1 kubectl get pods
vi readiness-pod-advanced.yaml
kubectl replace --force -f readiness-pod-advanced.yaml
watch -n1 kubectl get pods
kubectl describe pod readiness-pod-advanced 
cp -rp readiness-pod-advanced.yaml liveness-pod.yaml
vi liveness-pod.yaml
kubectl apply -f liveness-pod.yaml
watch -n1 kubectl get pods
kubectl describe pod liveness-pod 
watch -n1 kubectl get pods
kubectl describe pod liveness-pod 
watch -n1 kubectl get pods
kubectl exec -it liveness-pod -- bash
kubectl exec -it liveness-pod -c my-app -- bash
kubectl exec -it liveness-pod -c my-app -- sh
kubectl exec -it liveness-pod -c busybox -- bash
kubectl exec -it liveness-pod -c busybox -- sh
vi volume-pod.yaml
kubectl apply -f volume-pod.yaml
kubec get pod -o wide
kubectl get pod -o wide
ssh worker02
ssh 192.168.1.120
ssh 192.168.1.110
kubectl exec -it volume-pod --bash
kubectl exec -it volume-pod -- bash
ssh 192.168.1.110
ssh 192.168.1.120
kubectl delete pod volume-pod 
ssh 192.168.1.120
kubectl apply -f volume-pod.yaml
kubectl get pod -o wide
kubectl exec -it volume-pod -- bash
vi volume-pod.yaml 
kubectl delete pod volume-pod 
kubectl apply -f volume-pod.yaml
kubectl exec -it volume-pod -- bash
vi test-pv.yaml
kubectl get sc
vi test-pv.yaml
kubectl apply -f test-pv.yaml
kubectl  get pc
kubectl  get pv
vi test-pvc.yaml
kubectl apply -f test-pvc.yaml
kubectl  get pv,pvc
kubectl edit pvc test-pvc 
kubectl delete pvc test-pvc 
vi test-pvc.yaml 
kubectl apply -f test-pvc.yaml
kubectl  get pv,pvc
vi persistent-volume-pod.yaml
kubectl apply -f persistent-volume-pod.yaml
kubectl get pods
kubectl get pods -o wide
ssh 192.168.1.120
kubectl expose pod persistent-volume-pod --name pv-pod-svc --type NodePort --port 80
kubectl edit pod persistent-volume-pod 
kubectl expose pod persistent-volume-pod --name pv-pod-svc --type NodePort --port 80
kubectl get svc
kubectl delete pod persistent-volume-pod 
kubectl apply -f persistent-volume-pod.yaml
kubectl get svc,ep,pod
kubectl get svc,ep,pod -o wide
kubectl describe svc pv-pod-svc 
kubectl label pod persistent-volume-pod test=pv-pod
kubectl get svc,ep,pod -o wide
kubectl delete pod persistent-volume-pod 
kubectl delete pvc test-pvc 
kubectl get pv,pvc
kubectl apply -f test-pvc.yaml 
kubectl get pv,pvc
kubectl get pv test-pv -o yaml
kubectl patch pv test-pv -p '{"spec": {"claimRef": null}}'
kubectl get pv test-pv -o yaml
kubectl get pv,pvc
kubectl get sc
$ kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
kubectl get all -n local-path-storage
kubectl get sc
kubectl edit sc local-path 
kubectl get sc
kubectl get pv,pvc
kubectl delete pv test-pv 
kubectl get pv,pvc
kubectl edit pv test-pv 
kubectl get pv,pvc
kubectl delete pvc test-pvc 
kubectl get pv,pvc
cp -rp test-pvc.yaml local-pvc.yaml
vi local-pvc.yaml
kubectl apply -f local-pvc.yaml
kubectl get pv,pvc
cp -rp persistent-volume-pod.yaml pv-pod2.yaml
vi pv-pod2.yaml
kubectl delete pvc test-pvc 
vi local-pvc.yaml
kubectl apply -f local-pvc.yaml
kubectl get pv,pvc
kubectl apply -f pv-pod2.yaml
kubectl get pv,pvc
kubectl get pod pv-pod2
vi pv-pod2.yaml 
kubectl get pods
kubectl delete pod persistent-volume-pod 
kubectl get pv,pvc
kubectl delete pvc local-pvc 
kubectl get pv,pvc
vi test-pv.yaml 
kubectl get secrets -A
kubectl describe secrets -n kube-system cilium-ca
vi cm1
sudo cat /etc/ssh/sshd_config|grep -v "#"
vi sshd_config 
kubectl create cm config1 --from-file sshd_config 
kubectl get configmaps 
kubectl get configmaps config1 
kubectl get configmaps config1 -o yaml
vi envfile
kubectl create cm fromenvfile --from-env-file envfile 
kubectl describe cm fromenvfile 
kubectl create configmap my-configmap --from-literal mariadbpass=Secure123 --from-literal username=root --from-literal app_ver=2.0
kubectl describe configmaps my-configmap 
vi configmap-pod.yaml
kubectl apply -f configmap-pod.yaml
kubectl logs configmap-pod 
kubectl logs configmap-pod | grep -e TEMP
kubectl logs configmap-pod | grep -e VERSION
kubectl logs configmap-pod | grep -e VAR1
kubectl logs configmap-pod | grep -e SERVICE_B
kubectl  get cm
vi configmap-pod.yaml 
kubectl replace --force -f configmap-pod.yaml
kubectl exec  -it configmap-pod -- bash
kubectl exec  -it configmap-pod -- sh
htpasswd -c .htpasswd alina
sudo apt install apache2-utils -
htpasswd -c .htpasswd mariola
cat .htpasswd 
kubectl create secret generic nginx-htpasswd --from-file .htpasswd 
rm .htpasswd 
kubectl get secrets nginx-htpasswd -o yaml
echo "bWFyaW9sYTokYXByMSRxSTNJOWZvNCR4UEtxQ2JtUFdVVW1qMXVCOUEzZnExCg==" | base64 -d 
vi nginx-config.yaml
kubectl apply -f nginx-config.yaml
vi nginx-secure-pod.yaml
kubectl apply -f nginx-secure-pod.yaml
kubectl get pod nginx-secure-pod 
kubectl expose pod nginx-secure-pod --name secure-web-svc --port 80 --type NodePort
kubectl edit pod nginx-secure-pod 
kubectl expose pod nginx-secure-pod --name secure-web-svc --port 80 --type NodePort
kubectl  get svc
vi flask-app.yaml
kubectl get svc
vi flask-app.yaml
kubectl apply -f flask-app.yaml
kubectl get all -n app
watch -n1 kubectl get all -n app
kubectl -n app rollout restart deployment flask 
watch -n1 kubectl get all -n app
git statys
git status 
cd ..
git status 
git add 
git add .
git commit -m "adding day3 files"
git push
kubectl create namespace argocd 
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
watch -n1 kubectl get all -n argocd
kubectl -n argocd edit deployments.apps argocd-server 
kubectl patch deployment argocd-server -n argocd --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/args", "value": ["/usr/local/bin/argocd-server","--insecure"]}]'
vi argocd-ingress.yaml
kubectl -n argocd apply -f argocd-ingress.yaml
kubectl get ingress -n argocd 
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
kubectl -n argocd describe secrets argocd-initial-admin-secret 
kubectl -n argocd describe secrets argocd-initial-admin-secret -o yaml
kubectl -n argocd get secrets argocd-initial-admin-secret -o yaml
echo "WGt1MzVhZ2tJeExRUDVPbA==" | base64 -d 
ks
ls 
mkdir argocd
cd argocd/
cp -rp ../day3/configmap-pod.yaml  .
cp -rp ../day3/liveness-pod.yaml .
cp -rp ../day3/flask-app.yaml .
ls -al
git status
cd ..
git add .
git commit -m "add argocd content"
git push
kubectl get all -A
kubectl get all -n test-argo
history
history| awk '$1 > 562' | cut -c 8- > day3-history.txt
